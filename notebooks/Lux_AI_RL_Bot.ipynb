{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc6e611c",
   "metadata": {},
   "source": [
    "# Intelligent AI bot for a 2D game using Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d28c94e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> Please read through the report, especially Appendix-I before proceeding with the code as a lot of information has been provided about the codes. Several files have been referred to write this code from the luxai2021 package. Some testing codes although unused have been retained.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e641159c",
   "metadata": {},
   "source": [
    "## PPOBot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a68ed5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T03:43:27.370496Z",
     "start_time": "2023-05-01T03:43:27.354635Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> The below cell is written as PPOBot.py. This file is used in the main2.py shown in the appendix.\n",
    "    In order to run the below file in jupyter notebook, ensure that the %%writefile is commented out\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d03afa90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T00:20:00.779627Z",
     "start_time": "2023-05-02T00:19:59.411561Z"
    }
   },
   "outputs": [],
   "source": [
    "#%%writefile PPOBot.py \n",
    "# the above command is used to write the file PPOBot.py\n",
    "\n",
    "# Import packages\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import partial\n",
    "import logging\n",
    "from gym import spaces\n",
    "from luxai2021.env.agent import Agent\n",
    "from luxai2021.game.actions import *\n",
    "\n",
    "\n",
    "\n",
    "class PPOBot(Agent):\n",
    "    \"\"\"\n",
    "    This Class is a wrapper for AgentWithModel Class in agent.py.\n",
    "    Some functions have been modified and some have been directly used if there is no need for changes.\n",
    "    Major changes are present in the constructor, get_rewards(), get_observation() and game_start()\n",
    "    \"\"\"\n",
    "    # Constructor called to initialize variables\n",
    "    def __init__(self, mode=\"train\", model=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Below variables are required as a part of the Class as they are called during training\n",
    "        self.observation_space = []\n",
    "        self.model = model\n",
    "        self.mode = mode\n",
    "        \n",
    "        #self.actionSpaceUnits = []\n",
    "        #self.actionSpaceCities = []\n",
    "        # Below variables are used to define the action space. \n",
    "        # The action space is standardized for this game - each unit has 5 actions and each city as 3 actions\n",
    "        # The idea for using a partial function has been obtained from the file match_controller.py\n",
    "        self.actionSpaceUnits = [\n",
    "            partial(MoveAction, direction=Constants.DIRECTIONS.CENTER),\n",
    "            partial(MoveAction, direction=Constants.DIRECTIONS.NORTH),\n",
    "            partial(MoveAction, direction=Constants.DIRECTIONS.WEST),\n",
    "            partial(MoveAction, direction=Constants.DIRECTIONS.SOUTH),\n",
    "            partial(MoveAction, direction=Constants.DIRECTIONS.EAST),\n",
    "            SpawnCityAction]\n",
    "        self.actionSpaceCities = [SpawnWorkerAction, SpawnCartAction, ResearchAction]\n",
    "        \n",
    "        # Action space is either unit's action space or the city's action space\n",
    "        self.action_space = spaces.Discrete(max(len(self.actionSpaceUnits), len(self.actionSpaceCities)))\n",
    "        \n",
    "        # User defined variables\n",
    "        self.resource_list = []\n",
    "        self.unit_obs_shape = 37\n",
    "        self.obs_shape = 1+self.unit_obs_shape + 4 + 2 #+ 2 + (16*16*3)\n",
    "        self.last_turn_reward = 0\n",
    "        \n",
    "        # Observation space is common and a 47x1 matrix\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(self.obs_shape,), dtype=np.float16)\n",
    "\n",
    "    \n",
    "    def get_agent_type(self):\n",
    "        \"\"\"\n",
    "        Returns the type of agent. Use AGENT for inference, and LEARNING for training a model.\n",
    "        Code for this function obtained from agent.py. No changes needed\n",
    "        \"\"\"\n",
    "        if self.mode == \"train\":\n",
    "            return Constants.AGENT_TYPE.LEARNING\n",
    "        else:\n",
    "            return Constants.AGENT_TYPE.AGENT\n",
    "    \n",
    "    def game_start(self, game):\n",
    "        \"\"\"\n",
    "        This function is called at the start of each game. Use this to\n",
    "        reset and initialize per game. Note that self.team may have\n",
    "        been changed since last game. The game map has been created\n",
    "        and starting units placed.\n",
    "        \"\"\"\n",
    "        self.resources_last_turn = 0\n",
    "        self.cities_last_turn = 0 \n",
    "        self.units_last_turn = 0\n",
    "        self.research_points_last_turn = 0\n",
    "        self.dist_resource_last_turn = 0\n",
    "        self.max_cities = 0\n",
    "        self.fuel_last_turn = 0\n",
    "        self.city_created_flag = False\n",
    "        self.last_turn_unit_pos = []\n",
    "        self.last_turn_unit_cargo = []\n",
    "    \n",
    "    \n",
    "    def get_observation(self, game, unit, city_tile, team, is_new_turn):\n",
    "        \"\"\"\n",
    "        Implements getting a observation from the current game for this unit or city\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Call and initialize the self.resource_list variable\n",
    "        self.get_resources(game)\n",
    "        \n",
    "        # Observations related to Unit - for more details refer to get_unit_obs()\n",
    "        unit_obs = self.get_unit_obs(game, unit)\n",
    "        \n",
    "        # Observations related to Environment - for more details refer to get_env_obs()\n",
    "        env_obs = self.get_env_obs(game)\n",
    "        #opp_obs = self.get_opp_obs(game) # not used - opponent observations\n",
    "        \n",
    "        # Observations related to Resources - for more details refer to get_resource_obs()\n",
    "        resource_obs = self.get_resource_obs(game, unit) \n",
    "        \n",
    "        # Not used. Idea was to pass the map information in the observations. \n",
    "        # However, it did not yield good results possibly because of a sparse matrix\n",
    "        #map_info_obs = self.get_map_info(game)\n",
    "        \n",
    "        # Combine all observations\n",
    "        all_obs = []\n",
    "        all_obs.extend([1] if unit!=None else [0]) # Unit type\n",
    "        all_obs.extend(unit_obs) # 37\n",
    "        all_obs.extend(env_obs)  # 4\n",
    "        all_obs.extend(resource_obs) # 2 \n",
    "        \n",
    "        #all_obs.extend(opp_obs) # 2\n",
    "        #all_obs.extend(map_info_obs)\n",
    "        #print(all_obs)\n",
    "        #assert(len(all_obs)== 20)\n",
    "        #[self.find_nearest_resource(unit.pos.x, unit.pos.y)] #self.get_resource_obs(game, unit)\n",
    "        \n",
    "        return(all_obs)\n",
    "    \n",
    "    def get_unit_obs(self, game, unit):\n",
    "        \"\"\"\n",
    "        Observations related to Unit:\n",
    "        Current resource value\n",
    "        Distance from nearest city\n",
    "        Adjacent cell resources Y/N\n",
    "        Direction to nearest resource & city (OHE)\n",
    "        Distance to nearest resource & city\n",
    "        Adjacent cell units Y/N\n",
    "        Others (can act Y/N, current tile info etc.)\n",
    "        \"\"\"\n",
    "        if unit != None:\n",
    "            unit_obs = []\n",
    "            unit_obs.append(unit.type)\n",
    "            dirs = [(1,0), (0,1), (-1,0), (0,-1)]\n",
    "            \n",
    "            # check if nearby cell has any resources\n",
    "            for dir_ in dirs:\n",
    "                nearby_cell = game.map.get_cell(unit.pos.x+dir_[0], unit.pos.y+dir_[1])\n",
    "                if nearby_cell != None:\n",
    "                    if nearby_cell.resource != None:\n",
    "                        unit_obs.append(1)\n",
    "                    else:\n",
    "                        unit_obs.append(0)\n",
    "                else:\n",
    "                    unit_obs.append(0)\n",
    "            \n",
    "            # check if nearby cell has any units\n",
    "            for dir_ in dirs:\n",
    "                nearby_cell = game.map.get_cell(unit.pos.x+dir_[0], unit.pos.y+dir_[1])\n",
    "                \n",
    "                if nearby_cell != None:\n",
    "                    if nearby_cell.has_units() and nearby_cell.city_tile == None:\n",
    "                        unit_obs.append(1)\n",
    "                    else:\n",
    "                        unit_obs.append(0)\n",
    "                else:\n",
    "                    unit_obs.append(0)\n",
    "                \n",
    "            unit_cell = game.map.get_cell(unit.pos.x, unit.pos.y)\n",
    "            unit_obs.append(1 if unit_cell.is_city_tile() else 0)\n",
    "            unit_obs.append(1 if unit_cell.has_resource() else 0)\n",
    "            \n",
    "            unit_obs.append(unit.get_cargo_fuel_value()/100) # Some normalization wherever possible\n",
    "            unit_obs.append(unit.get_cargo_space_left()/100)\n",
    "            unit_obs.append(unit.get_light_upkeep()/100) #if game.is_night() else 0\n",
    "            unit_obs.append(1 if unit.can_build(game.map) else 0)\n",
    "            unit_obs.append(1 if unit.can_act() else 0)\n",
    "            unit_obs.append(self.get_nearest_city(game, unit.pos))\n",
    "            \n",
    "            # add direction to nearest resource\n",
    "            direction_list  = [0 for i in range(0,15)]\n",
    "            direction_wood = self.direction_to_nearest_resource(game, unit, \"wood\")\n",
    "            direction_coal = self.direction_to_nearest_resource(game, unit, \"coal\")\n",
    "            direction_uranium = self.direction_to_nearest_resource(game, unit, \"uranium\")\n",
    "            direction_list[direction_wood] = 1\n",
    "            direction_list[direction_coal+5] = 1\n",
    "            direction_list[direction_uranium+10] = 1\n",
    "            unit_obs.extend(direction_list)\n",
    "            \n",
    "            # add direction to nearest city\n",
    "            direction_list = [0,0,0,0,0]\n",
    "            direction = self.direction_to_nearest_city(game, unit)\n",
    "            direction_list[direction] = 1\n",
    "            unit_obs.extend(direction_list)\n",
    "            assert(len(unit_obs)== self.unit_obs_shape)\n",
    "            return(unit_obs)\n",
    "        else:\n",
    "            return([0 for x in range(self.unit_obs_shape)])\n",
    "    \n",
    "    def get_env_obs(self, game):\n",
    "        \"\"\"\n",
    "        Env observations:\n",
    "        Night Y/N, Turns to night\n",
    "        Research points\n",
    "        Number of units\n",
    "        -- can add in future - nearest unit from city tile\n",
    "        \"\"\"\n",
    "        \n",
    "        env_obs = []\n",
    "        \n",
    "        # Night time is important as bot needs to survive by consuming resources\n",
    "        env_obs.append(1 if game.is_night() else 0) \n",
    "        turns_to_night = (int(game.state['turn']/40)*40 + 40 - 10 - game.state['turn'])/40 # normalized by 40\n",
    "        if turns_to_night < 0:\n",
    "             turns_to_night = 0\n",
    "        env_obs.append(turns_to_night)\n",
    "        env_obs.append(game.state['teamStates'][0]['researchPoints']/20)\n",
    "        env_obs.append(len(game.state['teamStates'][0]['units'].keys())/10)\n",
    "        assert(len(env_obs)==4)\n",
    "        return(env_obs)\n",
    "    \n",
    "    def get_opp_obs(self, game):\n",
    "        \"\"\"\n",
    "        Resources related to the opponent. Not used in this project\n",
    "        \"\"\"\n",
    "        opp_obs = []\n",
    "        opp_obs.append(game.state['teamStates'][1]['researchPoints']/20)\n",
    "        opp_obs.append(len(game.state['teamStates'][1]['units'].keys())/10)\n",
    "        assert(len(opp_obs)==2)\n",
    "        return(opp_obs)\n",
    "    \n",
    "    def get_resource_obs(self, game, unit): \n",
    "        \"\"\"\n",
    "        Resource Observations:\n",
    "        Manhattan distance to nearest resource\n",
    "        -- can add in future: unit near nearest resource, amount of nearest resource\n",
    "        \"\"\"\n",
    "        resource_obs = []\n",
    "        resource_obs.append(self.find_nearest_resource(unit.pos.x, unit.pos.y) if unit!=None else 5)\n",
    "        resource_obs.append(game.stats[\"teamStats\"][self.team%2]['fuelGenerated']/100) # normalized by 100\n",
    "        #resource_obs.append(sum(game.stats[\"teamStats\"][self.team]['resourcesCollected'].values()))\n",
    "        assert(len(resource_obs)==2)\n",
    "        return(resource_obs)\n",
    "    \n",
    "    \n",
    "    def get_resources(self, game):\n",
    "        \"\"\"\n",
    "        This function obtains the resource type, cell location of all resources in the map\n",
    "        \"\"\"\n",
    "        self.resource_list = []\n",
    "        for i in range(game.map.width):\n",
    "            for j in range(game.map.height):\n",
    "                current_cell = game.map.get_cell(i, j)\n",
    "                if current_cell.has_resource():\n",
    "                    self.resource_list.append([current_cell.resource.type, i, j])\n",
    "    \n",
    "    def find_nearest_resource(self, x, y):\n",
    "        \"\"\"Find nearest resource using Manhattan distance\"\"\"\n",
    "        rl = np.array(self.resource_list, dtype=\"object\")\n",
    "        rl = rl[:,1:]\n",
    "        manhat = np.abs(np.array([x, y]) - np.array(rl[1:], dtype=\"int64\"))\n",
    "        nearest_resource_dist = np.min(np.sum(manhat, axis=1))\n",
    "        return(nearest_resource_dist)\n",
    "        \n",
    "\n",
    "    def get_actions(self, game):\n",
    "        return(0)\n",
    "        #return([self.actionSpaceUnits, self.actionSpaceCities])\n",
    "    \n",
    "    \n",
    "    def get_map_info(self, game):\n",
    "        \"\"\"\n",
    "        Not used. Idea was to pass the map information in the observations. \n",
    "        However, the model training could not be changed to keep a consistent map size.\n",
    "        \"\"\"\n",
    "        map_info_obs = []\n",
    "        #print(game.map.height, game.map.width)\n",
    "        for i in range(game.map.height):\n",
    "            for j in range(game.map.width):\n",
    "                cell = game.map.get_cell(i, j)\n",
    "                if cell.has_resource():\n",
    "                    map_info_obs.extend([1,0,0])\n",
    "                elif cell.is_city_tile():\n",
    "                    map_info_obs.extend([0,1,0])\n",
    "                elif cell.has_units():\n",
    "                    map_info_obs.extend([0,0,1])\n",
    "                else:\n",
    "                    map_info_obs.extend([0,0,0])\n",
    "        #print(game.map.height*game.map.width)\n",
    "        assert len(map_info_obs)==16*16*3#game.map.height*game.map.width\n",
    "        return(map_info_obs)\n",
    "    \n",
    "    def get_nearest_city(self, game, pos):\n",
    "        \"\"\"\n",
    "        Find the nearest city from the unit\n",
    "        \"\"\"\n",
    "        dist_list = [10]\n",
    "        cities = list(game.cities.values())\n",
    "        try:\n",
    "            if len(cities) > 0:\n",
    "                if len(cities[self.team%2].city_cells) > 0: # self.team%2 gets the team id\n",
    "                    for city in list(game.cities.values())[self.team%2].city_cells:\n",
    "                        manhat_dist = np.abs(city.pos.x - pos.x) + np.abs(city.pos.y - pos.y)\n",
    "                        dist_list.append(manhat_dist)\n",
    "            else:\n",
    "                return(10)\n",
    "        except:\n",
    "            pass\n",
    "        return(min(dist_list))\n",
    "\n",
    "    def direction_to_nearest_city(self, game, unit):\n",
    "        \"\"\"\n",
    "        Direction to nearest city - outputs 0,1,2,3,4 based on direction\n",
    "        \"\"\"\n",
    "        \n",
    "        # Idea for using the below dictionary obtained from https://www.kaggle.com/code/glmcdona/reinforcement-learning-openai-ppo-with-python-game\n",
    "        mapping = {\n",
    "                Constants.DIRECTIONS.CENTER: 0,\n",
    "                Constants.DIRECTIONS.NORTH: 1,\n",
    "                Constants.DIRECTIONS.WEST: 2,\n",
    "                Constants.DIRECTIONS.SOUTH: 3,\n",
    "                Constants.DIRECTIONS.EAST: 4,\n",
    "            }\n",
    "        dist = []\n",
    "        cities = game.cities.values()\n",
    "        for city in cities:\n",
    "            if city.team == unit.team:\n",
    "                #print(\"num cities:\", len(city.city_cells))\n",
    "                for citycell in city.city_cells:\n",
    "                    #print(\"City Pos:\", citycell.pos.x, citycell.pos.y)\n",
    "                    x_dif = np.abs(unit.pos.x - citycell.pos.x)\n",
    "                    y_dif = np.abs(unit.pos.y - citycell.pos.y)\n",
    "                    manhat = x_dif+y_dif\n",
    "                    dist.append(manhat)\n",
    "                min_dist_ind = np.argmin(np.array(manhat))\n",
    "                direction = unit.pos.direction_to(city.city_cells[min_dist_ind].pos)\n",
    "                #print(\"unit pos:\", unit.pos.x, unit.pos.y)\n",
    "                #print(\"direction: \", direction)\n",
    "                return(mapping[direction])\n",
    "        return(0)\n",
    "    \n",
    "    def direction_to_nearest_resource(self, game, unit, typ):\n",
    "        \"\"\"\n",
    "        Direction to nearest resource - outputs 0,1,2,3,4 based on direction\n",
    "        \"\"\"\n",
    "        mapping = {\n",
    "                Constants.DIRECTIONS.CENTER: 0,\n",
    "                Constants.DIRECTIONS.NORTH: 1,\n",
    "                Constants.DIRECTIONS.WEST: 2,\n",
    "                Constants.DIRECTIONS.SOUTH: 3,\n",
    "                Constants.DIRECTIONS.EAST: 4,\n",
    "            }\n",
    "        rl = np.array(self.resource_list, dtype=\"object\")\n",
    "        \n",
    "        #print(\"Before:\", len(rl))\n",
    "        #print(rl)\n",
    "        rl = rl[np.in1d(rl[:, 0], np.asarray([typ]))]\n",
    "        #print(\"After:\", len(rl))\n",
    "        #print(rl)\n",
    "        rl = rl[:,1:]\n",
    "        if len(rl)==0:\n",
    "            return(0)\n",
    "        manhat = np.abs(np.array([unit.pos.x, unit.pos.y]) - np.array(rl, dtype=\"int64\"))\n",
    "        nearest_resource_ind = np.argmin(np.sum(manhat, axis=1))\n",
    "        nearest_resource = self.resource_list[nearest_resource_ind][1:]\n",
    "        cell = game.map.get_cell(nearest_resource[0], nearest_resource[1])\n",
    "        direction = unit.pos.direction_to(cell.pos)\n",
    "        return(mapping[direction])    \n",
    "    \n",
    "    def get_reward(self, game, is_game_finished, is_new_turn, is_game_error):\n",
    "        \"\"\"\n",
    "        Returns the reward function for this step of the game. Reward should be a\n",
    "        delta increment to the reward, not the total current reward.\n",
    "        \n",
    "        All rewards are calcualted for just one team at a time\n",
    "        Below rewards are calculated:\n",
    "        Inc. Cities from last turn\n",
    "        Inc. Units from last turn\n",
    "        Inc. research points from last turn\n",
    "        Inc. resources from last turn\n",
    "        Unit roaming penalty (with full cargo)\n",
    "        Unit distance penalty from city at night\n",
    "        Penalty for not building city when cargo is full\n",
    "        Penalty for Inc. distance from resource from last turn\n",
    "        Penalty for building city at night\n",
    "        \n",
    "        Some rewards were tested but unused and retained to show progress\n",
    "        \"\"\"\n",
    "        \n",
    "        # Units in current turn\n",
    "        current_turn_unit_list = list(game.state[\"teamStates\"][0][\"units\"].values())\n",
    "        \n",
    "        if is_game_error == True:\n",
    "            return(-50)\n",
    "        \n",
    "        if is_new_turn == False and is_game_finished==False:\n",
    "            return(0)\n",
    "        \n",
    "        # Number of player units\n",
    "        player_units = len(game.state['teamStates'][self.team%2]['units'].keys())\n",
    "        \n",
    "        # Number of player cities\n",
    "        player_cities = 0\n",
    "        for city in game.cities.values():\n",
    "            if city.team == self.team:\n",
    "                player_cities +=1\n",
    "        #player_cities = len(game.cities.keys())\n",
    "        \n",
    "        # Player research points\n",
    "        player_research_points = game.state['teamStates'][self.team%2]['researchPoints']\n",
    "        \n",
    "        # Sum of distances from resources for all units\n",
    "        player_dist_resource = 0\n",
    "        for unit in current_turn_unit_list:\n",
    "            dist = self.find_nearest_resource(unit.pos.x, unit.pos.y)\n",
    "            if dist<=1:\n",
    "                dist = 0\n",
    "            if game.is_night()==True:\n",
    "                player_dist_resource += (1-dist)*20\n",
    "            else:\n",
    "                player_dist_resource += (1-dist)*5\n",
    "        \n",
    "        #print(\"Turn:\", game.state[\"turn\"], \"Res Dist:\", inc_dist_resource, \"Pen:\", inc_dist_resource)\n",
    "        \n",
    "        # sum (incremental resources from last turn)\n",
    "        player_total_resource = 0\n",
    "        for i in range(len(current_turn_unit_list)):\n",
    "            try:\n",
    "                player_resource_current = sum(current_turn_unit_list[i].cargo.values())\n",
    "                diff = (player_resource_current-self.last_turn_unit_cargo[i]) # last_turn_unit_cargo defined at end and initialized at game start\n",
    "                if diff == 100: # if diff is 100 then it has created a city. No need to penalize this\n",
    "                    diff = 0\n",
    "                player_total_resource += diff\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Unit roaming penalty and penalty for not building a city with full cargo\n",
    "        player_roaming_penalty = 0\n",
    "        not_build_city_penalty = 0\n",
    "        if len(self.last_turn_unit_pos) == len(current_turn_unit_list):\n",
    "            #print(\"same units\")\n",
    "            for i in range(len(current_turn_unit_list)):\n",
    "                if ((sum(current_turn_unit_list[i].cargo.values()) == 100) & (self.last_turn_unit_cargo[i] == 100)):\n",
    "                    if self.get_nearest_city(game, current_turn_unit_list[i].pos) > self.get_nearest_city(game, self.last_turn_unit_pos[i]):\n",
    "                        #print(\"farther\")\n",
    "                        player_roaming_penalty += 20\n",
    "                    unit_cell = game.map.get_cell(self.last_turn_unit_pos[i].x, self.last_turn_unit_pos[i].y)\n",
    "                    if (unit_cell.resource == None) & (unit_cell.city_tile==None):\n",
    "                        not_build_city_penalty +=40\n",
    "        \n",
    "        # Night time penalties - Units need to consume available resources at night or it will get eliminated\n",
    "        # distance from city and cargo space left (less cargo - more likely to get eliminated at night)\n",
    "        turns_to_night = int(game.state['turn']/40)*40 + 40 - 10 - game.state['turn']\n",
    "        player_dist_night = 0\n",
    "        player_night_space = 0\n",
    "        for unit in current_turn_unit_list:\n",
    "            player_dist_night += self.get_nearest_city(game, unit.pos)\n",
    "            player_night_space += 0.1*unit.get_cargo_space_left()\n",
    "        \n",
    "        # Increase penalties as night time approaches\n",
    "        if (turns_to_night >=5) and (turns_to_night<10):\n",
    "            player_dist_night *=2\n",
    "            player_night_space *=2\n",
    "        elif (turns_to_night >0) and (turns_to_night<5):\n",
    "            player_dist_night *=4\n",
    "            player_night_space *=4\n",
    "        elif turns_to_night==0:\n",
    "            player_dist_night *= 7\n",
    "            player_night_space *=7\n",
    "        else:\n",
    "            player_dist_night=0\n",
    "            player_night_space =0\n",
    "        \n",
    "        #print(\"Turn:\", game.state[\"turn\"], \"Night Dist:\",player_dist_night, \"Units:\", player_units)\n",
    "        \n",
    "        # Penalty for building a city at night time\n",
    "        # If a unit builds a city at night time, it loses those resources and it is left with no resources to survive the night\n",
    "        build_city_penalty = 0\n",
    "        if self.cities_last_turn < player_cities:\n",
    "            if turns_to_night<=3:\n",
    "                not_build_city_penalty=0\n",
    "                build_city_penalty +=50\n",
    "        \n",
    "        # Resources deposited in the city\n",
    "        player_fuel = game.stats[\"teamStats\"][self.team%2]['fuelGenerated']\n",
    "        #min_fuel = (player_fuel- 330) * 0.5\n",
    "        #print(min_fuel)\n",
    "            \n",
    "        \n",
    "        #if game.is_night()==True:\n",
    "            #player_total_resource = player_total_resource *1.5\n",
    "        \n",
    "        # Get Incremental rewards in turn\n",
    "        inc_cities = player_cities - self.cities_last_turn\n",
    "        inc_units = player_units - self.units_last_turn\n",
    "        inc_research_points = player_research_points - self.research_points_last_turn\n",
    "        inc_dist_resource = player_dist_resource - self.dist_resource_last_turn\n",
    "        inc_fuel = player_fuel - self.fuel_last_turn\n",
    "        inc_resources = player_total_resource - self.resources_last_turn\n",
    "        inc_resources = 0 if inc_resources < 0 else inc_resources # when building a city or consume resources during night\n",
    "        \n",
    "        #inc_fuel = 0 if inc_resources < 0 else inc_resources\n",
    "        #inc_cities = -0.1 if inc_cities < 0 else inc_cities\n",
    "        #inc_units = -0.1 if inc_units < 0 else inc_units\n",
    "        #inc_research_points = -1 if inc_research_points < 0 else inc_research_points\n",
    "        #print(\"player fuel:\", player_fuel)\n",
    "        #print(\"Last turn:\", self.fuel_last_turn)\n",
    "        #print(\"Inc:\", inc_fuel*2)\n",
    "        \n",
    "        # Total reward for current turn\n",
    "        player_current_reward = sum([\n",
    "                                     # Positive rewards\n",
    "                                     inc_cities*300,\n",
    "                                     inc_units*100,\n",
    "                                     inc_research_points*10,\n",
    "                                     inc_resources,\n",
    "                                     inc_fuel*2, # subs with inc fuel\n",
    "                                    \n",
    "                                     # Negative Rewards\n",
    "                                     -player_roaming_penalty,\n",
    "                                     -player_dist_night,\n",
    "                                     -player_night_space,\n",
    "                                     -build_city_penalty,\n",
    "                                     -not_build_city_penalty,\n",
    "                                     inc_dist_resource # already a negative value - refer player_dist_resource above\n",
    "                                    ])\n",
    "\n",
    "        \n",
    "        # Assign values for previous turn metrics to use for incremental calculations for next step\n",
    "        self.cities_last_turn = player_cities\n",
    "        self.units_last_turn = player_units\n",
    "        self.research_points_last_turn = player_research_points\n",
    "        self.dist_resource_last_turn = player_dist_resource\n",
    "        self.fuel_last_turn = player_fuel\n",
    "        self.resources_last_turn = player_total_resource\n",
    "        self.max_cities = max(self.max_cities, player_cities)\n",
    "        \n",
    "        self.last_turn_unit_cargo = []\n",
    "        self.last_turn_unit_pos = []\n",
    "        for unit in current_turn_unit_list:\n",
    "            self.last_turn_unit_pos.append(unit.pos)\n",
    "            self.last_turn_unit_cargo.append(sum(list(unit.cargo.values())))\n",
    "        \n",
    "        # Logs - used only for testing the codes and printing values\n",
    "        cities_a = sum([1 for city in game.cities.values() if city.team%2==0])\n",
    "        cities_b = sum([1 for city in game.cities.values() if city.team%1==0])\n",
    "        units_a = sum([1 for unit in game.state[\"teamStates\"][self.team%2][\"units\"].values()])\n",
    "        units_b = sum([1 for unit in game.state[\"teamStates\"][self.team%1][\"units\"].values()])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        if self.team==0:\n",
    "            print(\"End:\", round(player_current_reward, 1),\n",
    "                     \"Turn:\", game.state[\"turn\"], \n",
    "                     \"Cities:\", cities_a, cities_b,\n",
    "                      \"Units:\", units_a, units_b,\n",
    "                      \"MaxCity:\", self.max_cities,\n",
    "                      \"Res:\", player_research_points,\n",
    "                      \"city:\", inc_cities*300, \n",
    "                      \"Unit:\", inc_units*100, \n",
    "                      \"RewResh:\", inc_research_points*10, \n",
    "                      \"RR:\", round(inc_resources,1), \n",
    "                      \"Fuel:\", inc_fuel, \n",
    "                      \"Roam:\", -player_roaming_penalty, \n",
    "                      \"CityPen:\", -(build_city_penalty+not_build_city_penalty),\n",
    "                      \"Dist:\", -inc_dist_resource,\n",
    "                     )\n",
    "        \"\"\"\n",
    "        if is_game_finished and random.random() <= 0.05 and self.team==0:\n",
    "            print(\"End:\", round(player_current_reward, 1),\n",
    "                 \"Turn:\", game.state[\"turn\"], \n",
    "                  \"Seed:\", game.configs[\"seed\"],\n",
    "                 \"Cities:\", cities_a, cities_b,\n",
    "                  \"Units:\", units_a, units_b,\n",
    "                  \"MaxCity:\", self.max_cities,\n",
    "                  \"Res:\", player_research_points,\n",
    "                  \"city:\", inc_cities*300, \n",
    "                  \"Unit:\", inc_units*100, \n",
    "                  #\"RewResh:\", inc_research_points*10, \n",
    "                  #\"RewReso:\", round(inc_resources,1), \n",
    "                  \"Fuel:\", inc_fuel, \n",
    "                  #\"Roam:\", -player_roaming_penalty, \n",
    "                  \"CityPen:\", -(build_city_penalty+not_build_city_penalty),\n",
    "                  \"Dist:\", -inc_dist_resource,\n",
    "                  \"NSP:\", -player_night_space\n",
    "                 )\n",
    "        return(player_current_reward)\n",
    "    \n",
    "    \n",
    "    def process_turn(self, game, team):\n",
    "        \"\"\"\n",
    "        Obtained from agent.py directly. No changes needed\n",
    "        Decides on a set of actions for the current turn. Not used in training, only inference.\n",
    "        Returns: Array of actions to perform.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        actions = []\n",
    "        new_turn = True\n",
    "\n",
    "        # Inference the model per-unit\n",
    "        units = game.state[\"teamStates\"][team][\"units\"].values()\n",
    "        for unit in units:\n",
    "            if unit.can_act():\n",
    "                obs = self.get_observation(game, unit, None, unit.team, new_turn)\n",
    "                action_code, _states = self.model.predict(obs, deterministic=False)\n",
    "                if action_code is not None:\n",
    "                    actions.append(\n",
    "                        self.action_code_to_action(action_code, game=game, unit=unit, city_tile=None, team=unit.team))\n",
    "                new_turn = False\n",
    "\n",
    "        # Inference the model per-city\n",
    "        cities = game.cities.values()\n",
    "        for city in cities:\n",
    "            if city.team == team:\n",
    "                for cell in city.city_cells:\n",
    "                    city_tile = cell.city_tile\n",
    "                    if city_tile.can_act():\n",
    "                        obs = self.get_observation(game, None, city_tile, city.team, new_turn)\n",
    "                        action_code, _states = self.model.predict(obs, deterministic=False)\n",
    "                        if action_code is not None:\n",
    "                            actions.append(\n",
    "                                self.action_code_to_action(action_code, game=game, unit=None, city_tile=city_tile,\n",
    "                                                           team=city.team))\n",
    "                        new_turn = False\n",
    "\n",
    "        time_taken = time.time() - start_time\n",
    "        if time_taken > 0.5:  # Warn if larger than 0.5 seconds.\n",
    "            print(\"WARNING: Inference took %.3f seconds for computing actions. Limit is 1 second.\" % time_taken,\n",
    "                  file=sys.stderr)\n",
    "        \n",
    "        return actions\n",
    "    \n",
    "    \n",
    "    def action_code_to_action(self, action_code, game, unit=None, city_tile=None, team=None):\n",
    "        \"\"\"\n",
    "        Obtained from https://www.kaggle.com/code/glmcdona/reinforcement-learning-openai-ppo-with-python-game \n",
    "        as this is standardized and does not affect the model output. \n",
    "        \n",
    "        Takes an action in the environment according to actionCode:\n",
    "            actionCode: Index of action to take into the action array.\n",
    "        Returns: An action.\n",
    "        \"\"\"\n",
    "        # Map actionCode index into to a constructed Action object\n",
    "        try:\n",
    "            x = None\n",
    "            y = None\n",
    "            if city_tile is not None:\n",
    "                x = city_tile.pos.x\n",
    "                y = city_tile.pos.y\n",
    "            elif unit is not None:\n",
    "                x = unit.pos.x\n",
    "                y = unit.pos.y\n",
    "            \n",
    "            if city_tile != None:\n",
    "                action =  self.actionSpaceCities[action_code%len(self.actionSpaceCities)](\n",
    "                    game=game,\n",
    "                    unit_id=unit.id if unit else None,\n",
    "                    unit=unit,\n",
    "                    city_id=city_tile.city_id if city_tile else None,\n",
    "                    citytile=city_tile,\n",
    "                    team=team,\n",
    "                    x=x,\n",
    "                    y=y\n",
    "                )\n",
    "\n",
    "                # If the city action is invalid, default to research action automatically\n",
    "                if not action.is_valid(game, actions_validated=[]):\n",
    "                    action = ResearchAction(\n",
    "                        game=game,\n",
    "                        unit_id=unit.id if unit else None,\n",
    "                        unit=unit,\n",
    "                        city_id=city_tile.city_id if city_tile else None,\n",
    "                        citytile=city_tile,\n",
    "                        team=team,\n",
    "                        x=x,\n",
    "                        y=y\n",
    "                    )\n",
    "            else:\n",
    "                action =  self.actionSpaceUnits[action_code%len(self.actionSpaceUnits)](\n",
    "                    game=game,\n",
    "                    unit_id=unit.id if unit else None,\n",
    "                    unit=unit,\n",
    "                    city_id=city_tile.city_id if city_tile else None,\n",
    "                    citytile=city_tile,\n",
    "                    team=team,\n",
    "                    x=x,\n",
    "                    y=y\n",
    "                )\n",
    "            \n",
    "            return action\n",
    "        except Exception as e:\n",
    "            # Not a valid action\n",
    "            print(e)\n",
    "            return None\n",
    "    \n",
    "    def take_action(self, action_code, game, unit=None, city_tile=None, team=None):\n",
    "        \"\"\"\n",
    "        Obtained from https://www.kaggle.com/code/glmcdona/reinforcement-learning-openai-ppo-with-python-game \n",
    "        as this is standardized and does not affect the model output. \n",
    "        \n",
    "        Takes an action in the environment according to actionCode:\n",
    "        actionCode: Index of action to take into the action array.\n",
    "        \"\"\"\n",
    "        action = self.action_code_to_action(action_code, game, unit, city_tile, team)\n",
    "        self.match_controller.take_action(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321e7d4f",
   "metadata": {},
   "source": [
    "\n",
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "450f9855",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T00:20:30.148930Z",
     "start_time": "2023-05-02T00:20:24.814430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running in inference-only mode.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import packages for building the model here in jupyter notebooks\n",
    "import os\n",
    "import random\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "# luxai2021 is the folder name which needs to be copied to the same location as the notebook file\n",
    "from luxai2021.env.agent import Agent\n",
    "from luxai2021.env.lux_env import LuxEnvironment\n",
    "from luxai2021.game.constants import LuxMatchConfigs_Default\n",
    "from stable_baselines3.common.utils import get_schedule_fn\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.logger import configure\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "# Get the configuration for Lux AI\n",
    "configs = LuxMatchConfigs_Default\n",
    "\n",
    "# Opponent Agent\n",
    "#model = PPO.load(f\"models/model2.zip\")\n",
    "#opponent = PPOBot(mode=\"inference\", model = PPO.load(f\"models/model.zip\"))\n",
    "opponent = Agent()\n",
    "\n",
    "# Instantiate the PPOBot class in mode 'train'\n",
    "player = PPOBot(mode=\"train\")\n",
    "\n",
    "# Lux Environment for training\n",
    "env = LuxEnvironment(configs=configs, learning_agent=player, opponent_agent=opponent)\n",
    "\n",
    "# PPO Model\n",
    "model = PPO(\"MlpPolicy\", env,  learning_rate=0.001, gamma=0.9, n_steps=15000, verbose=1, batch_size = 1000,\n",
    "            tensorboard_log=\"./lux_tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"/\")\n",
    "\n",
    "# set up logger\n",
    "#tmp_path = \"C:\\\\MS in Data Science\\\\IIT\\\\Courses\\\\CS 584 - Machine Learning\\\\Project\\\\sb3_logs\\\\\"\n",
    "#new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "#model.set_logger(new_logger)\n",
    "\n",
    "# Learning rate schedule for step decay of sorts\n",
    "schedule = [\n",
    "    (2000000, 0.001), (4000000, 0.0005), (4000000, 0.0001) # step size and learning rate\n",
    "    #(1000, 0.001) # uncomment this part to test the code for a sample\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b408cf65",
   "metadata": {},
   "source": [
    "## Run Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52cec08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T04:34:57.169185Z",
     "start_time": "2023-05-01T04:34:57.141097Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> The model takes almost 4 hours to run. The saved model is present as 'model_final.zip' under LuxAI->models. If you need to run a sample change the variable 'schedule' in the above cell to run just 1000 iterations. It would run in a couple of minutes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aef66cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T00:20:58.829995Z",
     "start_time": "2023-05-02T00:20:37.191453Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./lux_tensorboard/20230501-192030/PPO_0\n",
      "End: -370 Turn: 33 Seed: None Cities: 0 1 Units: 0 0 MaxCity: 1 Res: 4 city: -300 Unit: -100 Fuel: 0 CityPen: 0 Dist: -20 NSP: 0\n",
      "End: -380 Turn: 271 Seed: None Cities: 0 1 Units: 0 0 MaxCity: 2 Res: 17 city: -300 Unit: -100 Fuel: 0 CityPen: 0 Dist: -10 NSP: 0\n",
      "End: -420 Turn: 77 Seed: None Cities: 0 1 Units: 0 0 MaxCity: 2 Res: 7 city: -300 Unit: -100 Fuel: 0 CityPen: 0 Dist: 20 NSP: 0\n",
      "End: -420 Turn: 34 Seed: None Cities: 0 1 Units: 0 0 MaxCity: 2 Res: 4 city: -300 Unit: -100 Fuel: 0 CityPen: 0 Dist: 20 NSP: 0\n",
      "End: -790 Turn: 31 Seed: None Cities: 0 1 Units: 0 0 MaxCity: 2 Res: 5 city: -600 Unit: -200 Fuel: 0 CityPen: 0 Dist: 0 NSP: 0\n",
      "End: -336 Turn: 116 Seed: None Cities: 0 1 Units: 0 0 MaxCity: 1 Res: 12 city: -300 Unit: -100 Fuel: 0 CityPen: 0 Dist: -60 NSP: 0\n",
      "End: -380 Turn: 71 Seed: None Cities: 0 1 Units: 0 0 MaxCity: 1 Res: 6 city: -300 Unit: -100 Fuel: 0 CityPen: 0 Dist: -10 NSP: 0\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94.1     |\n",
      "|    ep_rew_mean     | -2.2e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 829      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m steps, learning_rate \u001b[38;5;129;01min\u001b[39;00m schedule:\n\u001b[0;32m      9\u001b[0m     model\u001b[38;5;241m.\u001b[39mlr_schedule \u001b[38;5;241m=\u001b[39m get_schedule_fn(learning_rate) \u001b[38;5;66;03m# Learning rate schedule\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39msave(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/model.zip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:307\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    299\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    304\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    305\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:248\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    244\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 248\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:204\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    201\u001b[0m             terminal_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mpredict_values(terminal_obs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    202\u001b[0m         rewards[idx] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m terminal_value\n\u001b[1;32m--> 204\u001b[0m \u001b[43mrollout_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_last_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_last_episode_starts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;241m=\u001b[39m new_obs\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_episode_starts \u001b[38;5;241m=\u001b[39m dones\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:444\u001b[0m, in \u001b[0;36mRolloutBuffer.add\u001b[1;34m(self, obs, action, reward, episode_start, value, log_prob)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_starts[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(episode_start)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m--> 444\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_probs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m \u001b[43mlog_prob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Save checkpoint every 500K steps\n",
    "checkpoint_callback = CheckpointCallback(save_freq=500000, save_path='./models/', name_prefix=f'PPO_model_')\n",
    "\n",
    "#log_dir = dir_ + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#tb_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Model Training\n",
    "for steps, learning_rate in schedule:\n",
    "    model.lr_schedule = get_schedule_fn(learning_rate) # Learning rate schedule\n",
    "    model.learn(total_timesteps=steps, callback=[checkpoint_callback], reset_num_timesteps = False)\n",
    "\n",
    "# Save the model\n",
    "model.save(path=f'models/model.zip')\n",
    "\n",
    "print(\"Done training model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e4eb9e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T21:54:27.539759Z",
     "start_time": "2023-04-28T21:54:27.484192Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the final model\n",
    "# model.save(path=f'models/model_final.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6734ff4e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> In order to view the tensorboard logs, please refer to Appendix-I in the report. Tensorboard logs have been provided as a part of this report.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84f78d3",
   "metadata": {},
   "source": [
    "## Appendix - Only for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72295828",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> The below code is the main2.py file that is run in the command line prompt using the below code:     \n",
    "    \n",
    "        call lux-ai-2021 --seed=7 cs584/main2.py cs584/main2_reference.py --maxtime 10000\n",
    "</div>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3a3c0fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T04:49:57.889414Z",
     "start_time": "2023-05-01T04:49:57.842516Z"
    }
   },
   "source": [
    "from stable_baselines3 import PPO\n",
    "from luxai2021.env.agent import AgentFromStdInOut\n",
    "from luxai2021.env.lux_env import LuxEnvironment\n",
    "from luxai2021.game.constants import LuxMatchConfigs_Default\n",
    "from PPOBot import PPOBot\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    configs = LuxMatchConfigs_Default\n",
    "    model = PPO.load(f\"models/rl_model_1_5000000_steps.zip\")\n",
    "    opponent = AgentFromStdInOut()\n",
    "    player = PPOBot(mode=\"inference\", model=model)\n",
    "\n",
    "    # Run the environment\n",
    "    env = LuxEnvironment(configs, player, opponent)\n",
    "    env.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188a03c8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> The below code can be used to view summary results such as wins, cities built etc. for 10 games. This needs to be run after running the run_10_games.bat file as shown in Appendix-I. Please change the 'search_dir' below. For my pc, the replays (json files) were stored in \"C:\\Users\\username\\replays\\\"\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ce6e64d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T05:15:42.053309Z",
     "start_time": "2023-05-01T05:15:42.010581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of turns in each game: [111, 38, 112, 111, 71, 116, 111, 73, 358, 153]\n",
      "Avg Turns: 125.4\n",
      "Total cities built by player 1 (First agent passed as argument): [1, 1, 2, 2, 1, 3, 5, 3, 14, 4]\n",
      "Player1 Wins:  2\n",
      "Player2 Wins:  8\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "search_dir = \"C:\\\\Users\\\\shash\\\\replays\\\\\"\n",
    "files = list(filter(os.path.isfile, glob.glob(search_dir + \"*\")))\n",
    "files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "files = files[0:10]\n",
    "len_list = []\n",
    "cities_list = []\n",
    "player1_wins = 0\n",
    "\n",
    "for file in files:\n",
    "    f = open(file)\n",
    "    data = json.load(f)\n",
    "    len_list.append(len(data['allCommands']))\n",
    "    cities=0\n",
    "    for i in range(len(data['allCommands'])):\n",
    "        for d in data['allCommands'][i]:\n",
    "            if d['agentID']==0:\n",
    "                if \"bcity\" in d['command']:\n",
    "                    cities+=1\n",
    "    cities_list.append(cities)\n",
    "    if data['results']['ranks'][0]['agentID']==0:\n",
    "        player1_wins += 1\n",
    "    \n",
    "    \n",
    "print(\"Number of turns in each game:\", len_list)\n",
    "print(\"Avg Turns:\", sum(len_list)/len(len_list))\n",
    "print(\"Total cities built by player 1 (First agent passed as argument):\", cities_list)\n",
    "print(\"Player1 Wins: \", player1_wins)\n",
    "print(\"Player2 Wins: \", 10-player1_wins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c78e3b5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> The below codes are just for reference. You can ignore them\n",
    "</div>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22d19630",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T04:08:33.430514Z",
     "start_time": "2023-04-27T03:28:41.093332Z"
    }
   },
   "source": [
    "# This part is not used - idea was to continue training the model using the current model as the opponent\n",
    "# Continue training\n",
    "player = PPOBot(mode=\"train\")\n",
    "opponent = Agent()\n",
    "#opponent = Agent()\n",
    "env = LuxEnvironment(configs=configs,\n",
    "                     learning_agent=player,\n",
    "                     opponent_agent=opponent)\n",
    "schedule = [\n",
    "    (1000000, 0.0001),\n",
    "    (1000000, 0.0001),\n",
    "]\n",
    "model2 = PPO.load(f\"models/modelT14M.zip\", env=env, tensorboard_log=\"./lux_tensorboard/\")\n",
    "\n",
    "# Train the policy\n",
    "for steps, learning_rate in schedule:\n",
    "    model2.lr_schedule = get_schedule_fn(learning_rate)\n",
    "    model2.learn(total_timesteps=steps,\n",
    "                #callback=checkpoint_callback,\n",
    "                reset_num_timesteps = False)\n",
    "\n",
    "model2.save(path=f'models/model2.zip')\n",
    "\n",
    "print(\"Done training model.\")\n",
    "model2.save(path=f'models/model2.zip')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7b2cade6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T19:47:51.151710Z",
     "start_time": "2023-04-28T19:47:51.062396Z"
    },
    "collapsed": true
   },
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.results_plotter import load_results, plot_results\n",
    "\n",
    "# Load the training results from the log file\n",
    "tmp_path = \"C:\\\\MS in Data Science\\\\IIT\\\\Courses\\\\CS 584 - Machine Learning\\\\Project\\\\sb3_logs\\\\\"\n",
    "tmp_path = \"C:\\\\Users\\\\shash\\\\cs584\\\\lux_tensorboard\\\\20230426-234102\\\\PPO_0\"\n",
    "log_path = 'C:/Users/shash/cs584/lux_tensorboard/PPO_0/events.out.tfevents.1679421361.LAPTOP-B3ID0R18.1136.0'\n",
    "results = load_results(tmp_path)\n",
    "plot_results(results, \"PPO on Lux AI\", results.n_steps, results.x_axis, task_name=\"training\")\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b8ec7d4",
   "metadata": {},
   "source": [
    "tmp_path = \"C:\\\\MS in Data Science\\\\IIT\\\\Courses\\\\CS 584 - Machine Learning\\\\Project\\\\sb3_logs\\\\\"\n",
    "graph_data = pandas.read_csv(tmp_path + \"monitor.csv\")\n",
    "graph_data.tail()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9524558b",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(3, 2, figsize=(12, 8))\n",
    "axli = ax.flatten()\n",
    "sns.lineplot(x=graph_data['time/iterations'], y=graph_data['rollout/ep_len_mean'], ax=axli[0], legend='brief', label=\"Mean Episode length\")\n",
    "sns.lineplot(x=graph_data['time/iterations'], y=graph_data['train/explained_variance'], ax=axli[1], legend='brief', label=\"Explained Variance\")\n",
    "sns.lineplot(x=graph_data['time/iterations'], y=graph_data['train/entropy_loss'], ax=axli[2], legend='brief', label=\"Entropy Loss\")\n",
    "sns.lineplot(x=graph_data['time/iterations'], y=graph_data['train/loss'], ax=axli[3], legend='brief', label=\"Loss\")\n",
    "sns.lineplot(x=graph_data['time/iterations'], y=graph_data['train/policy_gradient_loss'], ax=axli[4], legend='brief', label=\"Policy Grad Loss\")\n",
    "sns.lineplot(x=graph_data['time/iterations'], y=graph_data['train/approx_kl'], ax=axli[5], legend='brief', label=\"Approx KL\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b31d9fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T00:22:21.935425Z",
     "start_time": "2023-04-27T00:22:21.762659Z"
    }
   },
   "source": [
    "sns.lineplot(x=graph_data['time/iterations'], y=graph_data['rollout/ep_len_mean'], legend='brief', label=\"Mean Episode length\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "121a105a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T00:36:43.277985Z",
     "start_time": "2023-04-27T00:36:43.258366Z"
    }
   },
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "raw",
   "id": "891893ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T00:37:37.884033Z",
     "start_time": "2023-04-27T00:37:37.821261Z"
    }
   },
   "source": [
    "import datetime\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
